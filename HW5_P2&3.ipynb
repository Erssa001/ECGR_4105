{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLYjGsQxUK4ldqscDn7DrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erssa001/ECGR_4105/blob/main/HW5_P2%263.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAFTpNZUe8fp",
        "outputId": "9d4680cb-35a7-421b-af46-3f1f2533760e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/2023-2024/ECGR_4105/Housing.csv'\n",
        "dataset = pd.DataFrame(pd.read_csv(file_path))\n",
        "\n",
        "bi_vars =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "\n",
        "# Defining the map function\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Applying the function to the housing list\n",
        "dataset[bi_vars] = dataset[bi_vars].apply(binary_map)\n",
        "# Preprocessing\n",
        "\n",
        "# uncomment line below for problem 2\n",
        "#components_to_use = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "# uncomment line below for problem 3\n",
        "components_to_use = ['area', 'bedrooms','bathrooms','stories','mainroad','guestroom','basement','hotwaterheating','airconditioning', 'parking', 'prefarea']\n",
        "X = torch.tensor(dataset[components_to_use].values.copy())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "dataset[num_vars] = scaler.fit_transform(dataset[num_vars])\n",
        "X_n = torch.tensor(dataset[components_to_use].values.copy())\n",
        "\n",
        "Y = torch.tensor(dataset.pop('price').values)"
      ],
      "metadata": {
        "id": "kxxY55ZIfzrt"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = X.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "X_train = X[train_indices]\n",
        "Y_train = Y[train_indices]\n",
        "\n",
        "X_test = X[val_indices]\n",
        "Y_test = Y[val_indices]\n",
        "\n",
        "X_n_train = X_n[train_indices]\n",
        "X_n_test = X_n[val_indices]"
      ],
      "metadata": {
        "id": "gnkQwZJkdC2q"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, *params):\n",
        "  result = params[-1]\n",
        "  for i in range(0, X.shape[1]):\n",
        "    result = result + (X[:, i] * params[i])\n",
        "  return result\n",
        "\n",
        "def loss_fn(Y_pred, Y):\n",
        "  squared_diffs = (Y_pred - Y)**2\n",
        "  return squared_diffs.mean()\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, X_train, X_test, Y_train, Y_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    Y_pred_train = model(X_train, *params)\n",
        "\n",
        "    train_loss = loss_fn(Y_pred_train, Y_train)\n",
        "\n",
        "    with torch.no_grad(): # <1>\n",
        "      Y_pred_test = model(X_test, *params)\n",
        "      val_loss = loss_fn(Y_pred_test, Y_test)\n",
        "      assert val_loss.requires_grad == False # <2>\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    bias_value = params[-1].item()\n",
        "    if ((epoch == 1) or (epoch % 500 == 0)):\n",
        "      print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f}, Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "P5GQIVm4OtXo"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.ones(X.shape[1] + 1, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "  params[-1] = 0\n",
        "learning_rate = .1\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "SGD_linear = training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    X_train = X_n_train, # <1>\n",
        "    X_test = X_n_test, # <1>\n",
        "    Y_train = Y_train,\n",
        "    Y_test = Y_test)\n",
        "\n",
        "params = torch.ones(X.shape[1] + 1, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "  params[-1] = 0\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.Adam([params], lr=learning_rate)\n",
        "\n",
        "adam_linear = training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    X_train = X_n_train, # <1>\n",
        "    X_test = X_n_test, # <1>\n",
        "    Y_train = Y_train,\n",
        "    Y_test = Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaqcaxpsXSTK",
        "outputId": "51c2b06a-cfed-4c31-f078-ca9d19ad6d4c"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 12.7202, Validation loss 11.4154\n",
            "Epoch 500, Training loss 0.3366, Validation loss 0.2967\n",
            "Epoch 1000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 1500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 2000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 2500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 3000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 3500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 4000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 4500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 5000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 1, Training loss 12.7202, Validation loss 11.4154\n",
            "Epoch 500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 1000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 1500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 2000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 2500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 3000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 3500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 4000, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 4500, Training loss 0.3366, Validation loss 0.2965\n",
            "Epoch 5000, Training loss 0.3366, Validation loss 0.2965\n"
          ]
        }
      ]
    }
  ]
}